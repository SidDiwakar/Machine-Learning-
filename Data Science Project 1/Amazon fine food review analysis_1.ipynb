{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon fine food review analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data using sqlite3\n",
    "\n",
    "con = sqlite3.connect('D:\\\\Data Science\\\\Dataset\\\\amazon-fine-food-reviews\\\\database.sqlite')\n",
    "\n",
    "\n",
    "# filtering only positive and negative review not taking the value which has review as 3\n",
    "\n",
    "filtered_data  = pd.read_sql_query(\"\"\"\n",
    "select * \n",
    "from Reviews\n",
    "where Score !=3\n",
    "\"\"\", con)\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give the score > 3 positive and score<3 negative\n",
    "\n",
    "def partition(x):\n",
    "    if x > 3:\n",
    "        return 'Positive'\n",
    "    return 'Negative'\n",
    "\n",
    "# changing the review with score less than 3 to be positive and vice ve\n",
    "\n",
    "\n",
    "\n",
    "rsa\n",
    "# positiveNegative = filtered_data['Score'].map(partition)\n",
    "filtered_data['Score'] = filtered_data['Score'].map(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('the shape of the data is : ',filtered_data.shape)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data cleaning : deduplication of the data \n",
    "\n",
    "display = pd.read_sql_query(\"\"\"\n",
    "select * from Reviews\n",
    "where Score !=3 and UserId =\"AR5J8UI46CURR\"\n",
    "order by ProductId\n",
    "\"\"\", con)\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shorting the data according to the ProductId\n",
    "\n",
    "shoted_data = filtered_data.sort_values(\"ProductId\", axis=0, ascending=True)\n",
    "\n",
    "# Deduplication of entries\n",
    "final = shoted_data.drop_duplicates(subset ={'UserId','ProfileName','Time','Text'},keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Checking the remaining % data left \n",
    "\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "    it was also seen that two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator\n",
    "    which is not practically possible  hence these two rows are also removed from calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "select *\n",
    "from Reviews\n",
    "where score !=3 AND Id = 44737 OR Id= 64422\n",
    "order by ProductID\n",
    "\"\"\", con)\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many Positive and Negative reviews are present in our dataset\n",
    "\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() # in scikit learn\n",
    "final_count = count_vect.fit_transform(final['Text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(final_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing : \n",
    "    Stemming , stop word removel and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # python regular expression\n",
    "i=0\n",
    "for sent in final['Text'].values:\n",
    "    if(len(re.findall('<.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english'))  # set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') # initilizing the snow ball stmmer\n",
    "\n",
    "def cleanhtml(sentence):     #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr,\" \", sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): # function to clean the word of any punctuation\n",
    "    cleaned  = re.sub(r'[? |!|\\'|\"|#]', r'',sentence)\n",
    "    cleaned  = re.sub(r'[. |,|)|(|\\|/]', r'',cleaned)\n",
    "    return cleaned\n",
    "print(stop)\n",
    "print(\"**********************************************************************\")\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for implementing step-by-step gthe checks mentioned in the pre-processing \n",
    "# this code takes a while to run as it has to run on 500k sentences. \n",
    "\n",
    "i= 0 \n",
    "str1  = ' '\n",
    "final_string = []\n",
    "all_positive_words = [] #store words from positive reviews here\n",
    "all_negative_words = [] #store words from negative reviews here\n",
    "s=''\n",
    "\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "#     print(sent)\n",
    "    sent = cleanhtml(sent) #remove html tags\n",
    "    \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_word.isalpha()) & (len(cleaned_words)>2)):\n",
    "                if (cleaned_words.lower() not in stop):\n",
    "                    s= (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    \n",
    "                    if (final['Score'].values)[i] == 'Positive':\n",
    "                        all_positive_words.append(s)  # list of all the words which is positive\n",
    "                        \n",
    "                    if(final['Score'].values)[i] == \"Negative\":\n",
    "                        all_negative_words.append(s)  # list of all the words which is negative\n",
    "                        \n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    #                 print(filtered_sentences)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    # print(\"*******************************************************************************\")\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final['CleanedText'] = final_string # adding a column of clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.head(3) # below the processed view can be seen in the cleaned_text\n",
    "\n",
    "# Store final table into an Sqlite table for future.\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews', conn, flavor = None, schema = None, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Grams and n-Grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Motivation:\n",
    "    now that we have our list of words describing positive and negative reviews lets analyse them.\n",
    "    We begin analysis by getting the frequency distribution of the words as shown below\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_dist_positive = nltk.FreqDist(all_positive_words)\n",
    "frequency_dist_negative = nltk.FreqDist(all_negative_words)\n",
    "\n",
    "print(\"most common positive words : \",frequency_dist_positive.most_common(20))\n",
    "print(\"most common negative words : \",frequency_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bi-gram , tri-gram and n-gram\n",
    "\n",
    "# removing stop words like \"not\" should be avoided before building the n-grams\n",
    "count_vect = CountVectorizer(ngram_range = (1,2)) # in scikit learn 1= give me the UNIGRAMS; 2 = up to BIGRAMS\n",
    "final_bigram_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_bigram_counts.get_shape() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range = (1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert a raw in sparsematrix to a numpy array\n",
    "print(final_tf_idf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row,features, top_n =25):\n",
    "    \"\"\"get top tf idf values in row and return them with their corrospond\"\"\"\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i],row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    \n",
    "    df.columns = ['features', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0],features, 25)\n",
    "\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
